{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Energy-Based Learning Schemes\n",
    "# ==============================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, name, external_world, hyper, \n",
    "                 neg_phase, pos_phase,\n",
    "                 activation=torch.sigmoid, energy_fn=None, relax_fn=None):\n",
    "        self.external_world = external_world\n",
    "        self.hyper = hyper\n",
    "        sizes = [external_world.x.shape[1]] + hyper[\"hidden_sizes\"] + [hyper.get(\"output_size\", len(torch.unique(external_world.y)))]\n",
    "        self.biases = [torch.zeros(s) for s in sizes]\n",
    "        self.weights = [torch.empty((a, b)).uniform_(-np.sqrt(6/(a+b)), np.sqrt(6/(a+b))) for a, b in zip(sizes[:-1], sizes[1:])]\n",
    "        self.persistent = [torch.zeros(external_world.size_dataset, s) for s in sizes[1:]]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.index = 0\n",
    "        self.activation = activation\n",
    "        self.energy_fn = energy_fn\n",
    "        self.neg_phase = neg_phase\n",
    "        self.pos_phase = pos_phase\n",
    "        self.relax_fn = relax_fn\n",
    "        if(\"beta\" in hyper):\n",
    "            self.beta = hyper[\"beta\"]\n",
    "\n",
    "    def update_mini_batch_index(self, i):\n",
    "        b = self.batch_size\n",
    "        self.index = i\n",
    "        self.x = self.external_world.x[i*b:(i+1)*b]\n",
    "        self.y = self.external_world.y[i*b:(i+1)*b]\n",
    "        self.y_onehot = F.one_hot(self.y, num_classes=self.weights[-1].shape[1]).float()\n",
    "        self.layers = [self.x] + [p[i*b:(i+1)*b] for p in self.persistent]\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - x.min(0)[0]) / (x.max(0)[0] - x.min(0)[0] + 1e-5) - x.mean(0)\n",
    "\n",
    "    def energy(self, layers):\n",
    "        return self.energy_fn(layers, self.weights, self.biases) if self.energy_fn else torch.tensor(0.)\n",
    "\n",
    "    def cost(self, layers):\n",
    "        return ((layers[-1] - self.y_onehot) ** 2).sum(1)\n",
    "\n",
    "    def measure(self):\n",
    "        E = self.energy(self.layers).mean().item()\n",
    "        C = self.cost(self.layers).mean().item()\n",
    "        top_vals, top_idxs = self.layers[-1].topk(2, dim=1)\n",
    "        err = (~((top_idxs[:,0] == self.y) & (top_vals[:,0] - top_vals[:,1] > 1e-3))).float().mean().item()\n",
    "        return E, C, err\n",
    "\n",
    "    def forward(self, batch, iters):\n",
    "        self.x, self.y = batch\n",
    "        self.y_onehot = F.one_hot(self.y, num_classes=self.weights[-1].shape[1]).float()\n",
    "        self.layers = [self.x] + [p[:self.x.size(0)] for p in self.persistent]\n",
    "        self.neg_phase(self, iters)\n",
    "        return self.measure()\n",
    "\n",
    "    def backward(self, output, iters=10, clamped=[-1]):\n",
    "        layers = self.layers.copy(); layers[-1] = output\n",
    "        layers = self.relax_fn(self, layers, iters, clamped) if self.relax_fn else layers\n",
    "        s, e = self.index * self.batch_size, (self.index + 1) * self.batch_size\n",
    "        for i, l in enumerate(layers[1:]):\n",
    "            self.persistent[i][s:e] = l.detach()\n",
    "        self.layers = [self.x] + [p[s:e] for p in self.persistent]\n",
    "        return layers[0]\n",
    "\n",
    "    def train(self, plot=False):\n",
    "        hist = {\"Energy\": [], \"Cost\": [], \"Error\": []}\n",
    "        EPOCHS = self.hyper[\"n_epochs\"]\n",
    "        ALPHAS = self.hyper[\"alphas\"]\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in range(self.external_world.size_dataset // self.batch_size):\n",
    "                self.update_mini_batch_index(i)\n",
    "                self.neg_phase(self, self.hyper[\"n_it_neg\"])\n",
    "                self.pos_phase(self, self.hyper[\"n_it_pos\"], *ALPHAS)\n",
    "            E, C, err = self.measure()\n",
    "            hist[\"Energy\"].append(E)\n",
    "            hist[\"Cost\"].append(C)\n",
    "            hist[\"Error\"].append(err * 100)\n",
    "            # print(f\"Epoch {epoch+1}/{EPOCHS} | E={E:.2f} C={C:.5f} Error={err*100:.2f}%\")\n",
    "            if plot and epoch in [0, EPOCHS-1]:\n",
    "                self.plot_weights_verbose(epoch)\n",
    "        if plot:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "            for ax, (k, c) in zip(axes, [(\"Energy\", \"blue\"), (\"Cost\", \"orange\"), (\"Error\", \"red\")]):\n",
    "                ax.plot(hist[k], label=k, color=c)\n",
    "                ax.set_title(f\"{k} over Epochs\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "        return np.mean(hist[\"Error\"][-4:])\n",
    "    \n",
    "\n",
    "    def plot_weights_verbose(self, epoch=None, sample_idx=0):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import networkx as nx\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        labels = ['Input'] + [f'Hidden {i+1}' for i in range(len(self.hyper[\"hidden_sizes\"]))] + ['Output']\n",
    "        sizes = [self.weights[0].shape[0]] + [w.shape[1] for w in self.weights]\n",
    "\n",
    "        # Collect activations and biases\n",
    "        if hasattr(self, 'layers'):\n",
    "            activations = [layer[sample_idx].detach().cpu().numpy() for layer in self.layers]\n",
    "        else:\n",
    "            activations = [np.zeros(s) for s in sizes]\n",
    "        biases = [b.detach().cpu().numpy() for b in self.biases]\n",
    "\n",
    "        # Add nodes\n",
    "        for layer_idx, size in enumerate(sizes):\n",
    "            for node_idx in range(size):\n",
    "                label = f\"{labels[layer_idx]} {node_idx}\\nact={activations[layer_idx][node_idx]:.2f}\\nbias={biases[layer_idx][node_idx]:.2f}\"\n",
    "                G.add_node((layer_idx, node_idx), label=label, layer=layer_idx, act=activations[layer_idx][node_idx])\n",
    "\n",
    "        # Add edges with weight\n",
    "        edge_labels = {}\n",
    "        edge_weights = []\n",
    "        for layer_idx, W in enumerate(self.weights):\n",
    "            for i in range(W.shape[0]):\n",
    "                for j in range(W.shape[1]):\n",
    "                    weight = W[i, j].item()\n",
    "                    u = (layer_idx, i)\n",
    "                    v = (layer_idx + 1, j)\n",
    "                    G.add_edge(u, v, weight=weight)\n",
    "                    edge_labels[(u, v)] = f\"{weight:.2f}\"\n",
    "                    edge_weights.append(weight)\n",
    "\n",
    "        # Normalize edge weights for coloring\n",
    "        edge_weights = np.array(edge_weights)\n",
    "        norm = plt.Normalize(vmin=-np.abs(edge_weights).max(), vmax=np.abs(edge_weights).max())\n",
    "        cmap = plt.cm.RdBu\n",
    "\n",
    "        # Layout and drawing\n",
    "        pos = nx.multipartite_layout(G, subset_key='layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, ax=ax,\n",
    "            edge_color=edge_weights,\n",
    "            edge_cmap=cmap,\n",
    "            edge_vmin=norm.vmin,\n",
    "            edge_vmax=norm.vmax,\n",
    "            width=2.0,\n",
    "            connectionstyle='arc3,rad=0.15',\n",
    "            arrowsize=12\n",
    "        )\n",
    "\n",
    "        # Draw nodes with activation as color\n",
    "        node_colors = [G.nodes[n]['act'] for n in G.nodes]\n",
    "        nodes = nx.draw_networkx_nodes(G, pos, ax=ax, node_size=1000, node_color=node_colors, cmap=plt.cm.viridis)\n",
    "\n",
    "        # Draw labels on nodes\n",
    "        node_labels = {n: G.nodes[n]['label'] for n in G.nodes}\n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=7)\n",
    "\n",
    "        # Draw edge labels (weights)\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
    "\n",
    "        # Colorbar for edges\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array(edge_weights)\n",
    "        fig.colorbar(sm, ax=ax, label=\"Weight\")\n",
    "\n",
    "        title = f\"Network Visualization (Epoch {epoch}\" if epoch is not None else \"Network Visualization\"\n",
    "        title += f\", Sample {sample_idx})\"\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Activation / Normalization ----------\n",
    "def normalize(x):\n",
    "    return (x - x.min(0)[0]) / (x.max(0)[0] - x.min(0)[0] + 1e-5) - x.mean(0)\n",
    "\n",
    "# ---------- Relaxation Dynamics (for CHL/EP/PC) ----------\n",
    "def relax(network, layers, iters, reverse=False, clamped=None):\n",
    "    for _ in range(iters):\n",
    "        new = [layers[-1]] if reverse else [layers[0]]\n",
    "        rng = range(len(layers)-2, 0, -1) if reverse else range(1, len(layers)-1)\n",
    "        for k in rng:\n",
    "            a = network.layers[k-1] if reverse else new[-1]\n",
    "            b = new[-1] if reverse else layers[k+1]\n",
    "            w1, w2 = network.weights[k - 1], network.weights[k]\n",
    "            try:\n",
    "                h = network.activation(a @ w1 + b @ w2.T + network.biases[k])\n",
    "            except RuntimeError as e:\n",
    "                if 'mat1 and mat2 shapes cannot be multiplied' in str(e):\n",
    "                    raise RuntimeError(f\"Shape mismatch at layer {k}: a={a.shape}, w1={w1.shape}, b={b.shape}, w2.T={w2.T.shape}\") from e\n",
    "                else:\n",
    "                    raise\n",
    "            new.append(normalize(h))\n",
    "        new.append(network.layers[0] if reverse else network.y_onehot)\n",
    "        layers = list(reversed(new)) if reverse else [network.x] + new[1:]\n",
    "    return layers\n",
    "\n",
    "# ---------- CHL ----------\n",
    "def chl_neg(network, steps):\n",
    "    layers = [x.clone() for x in network.layers]\n",
    "    for _ in range(steps):\n",
    "        new = [layers[0]]\n",
    "        for k in range(1, len(layers) - 1):\n",
    "            h = network.activation(\n",
    "                new[-1] @ network.weights[k - 1] + \n",
    "                layers[k + 1] @ network.weights[k].T + \n",
    "                network.biases[k]\n",
    "            )\n",
    "            new.append(network.normalize(h))\n",
    "        out = network.activation(new[-1] @ network.weights[-1] + network.biases[-1])\n",
    "        new.append(out)\n",
    "        layers = [layers[0]] + new[1:]\n",
    "    s, e = network.index * network.batch_size, (network.index + 1) * network.batch_size\n",
    "    for i, l in enumerate(layers[1:]):\n",
    "        network.persistent[i][s:e] = l.detach()\n",
    "    network.layers = [network.x] + [p[s:e] for p in network.persistent]\n",
    "\n",
    "def chl_pos(network, steps, *alphas):\n",
    "    clamped = relax(network, network.layers[:-1] + [network.y_onehot], steps, reverse=True)\n",
    "    for i, delta in enumerate([c - o for c, o in zip(clamped[1:], network.layers[1:])]):\n",
    "        if i + 1 < len(network.biases):\n",
    "            network.biases[i + 1] += alphas[i] * delta.mean(0)\n",
    "        if i < len(network.weights):\n",
    "            network.weights[i] += alphas[i] * network.layers[i].T @ delta / network.batch_size\n",
    "\n",
    "# ---------- Equilibrium Propagation (EP) ----------\n",
    "def ep_neg(network, steps):\n",
    "    layers = [x.clone() for x in network.layers]\n",
    "    for _ in range(steps):\n",
    "        new = [layers[0]]\n",
    "        for k in range(1, len(layers) - 1):\n",
    "            h = network.activation(\n",
    "                new[-1] @ network.weights[k - 1] + \n",
    "                layers[k + 1] @ network.weights[k].T + \n",
    "                network.biases[k]\n",
    "            )\n",
    "            new.append(network.normalize(h))\n",
    "        out = network.activation(new[-1] @ network.weights[-1] + network.biases[-1])\n",
    "        out -= network.beta * (out - network.y_onehot)\n",
    "        new.append(out)\n",
    "        layers = [layers[0]] + new[1:]\n",
    "    s, e = network.index * network.batch_size, (network.index + 1) * network.batch_size\n",
    "    for i, l in enumerate(layers[1:]):\n",
    "        network.persistent[i][s:e] = l.detach()\n",
    "    network.layers = [network.x] + [p[s:e] for p in network.persistent]\n",
    "\n",
    "def ep_pos(network, steps, *alphas):\n",
    "    clamped = relax(network, network.layers, steps)\n",
    "    for i, delta in enumerate([c - o for c, o in zip(clamped[1:], network.layers[1:])]):\n",
    "        if i + 1 < len(network.biases):\n",
    "            network.biases[i + 1] += alphas[i] * delta.mean(0)\n",
    "        if i < len(network.weights):\n",
    "            network.weights[i] += alphas[i] * network.layers[i].T @ delta / network.batch_size\n",
    "\n",
    "# ---------- First-Step Gradient Approximation ----------\n",
    "def first_step_pos(network, steps, *alphas):\n",
    "    layers = [x.clone() for x in network.layers]\n",
    "    out = layers[-2] @ network.weights[-1] + network.biases[-1]\n",
    "    nudged = network.activation(out - network.beta * (network.activation(out) - network.y_onehot))\n",
    "    delta = nudged - network.activation(out)\n",
    "    for i in range(len(network.weights)):\n",
    "        if i + 1 < len(network.biases):\n",
    "            network.biases[i + 1] += alphas[i] * delta.mean(0)\n",
    "        network.weights[i] += alphas[i] * network.layers[i].T @ delta / network.batch_size\n",
    "\n",
    "# ---------- PC-Nudge ----------\n",
    "def pcnudge_neg(network, steps):\n",
    "    layers = [x.clone() for x in network.layers]\n",
    "    for _ in range(steps):\n",
    "        new = [layers[0]]\n",
    "        for k in range(1, len(layers) - 1):\n",
    "            pred = network.activation(new[-1] @ network.weights[k - 1] + network.biases[k])\n",
    "            error = layers[k] - pred\n",
    "            new.append(network.normalize(pred + error))\n",
    "        out = network.activation(new[-1] @ network.weights[-1] + network.biases[-1])\n",
    "        out -= network.beta * (out - network.y_onehot)\n",
    "        new.append(out)\n",
    "        layers = [layers[0]] + new[1:]\n",
    "    s, e = network.index * network.batch_size, (network.index + 1) * network.batch_size\n",
    "    for i, l in enumerate(layers[1:]):\n",
    "        network.persistent[i][s:e] = l.detach()\n",
    "    network.layers = [network.x] + [p[s:e] for p in network.persistent]\n",
    "\n",
    "def pcnudge_pos(network, steps, *alphas):\n",
    "    clamped = relax(network, network.layers, steps)\n",
    "    for i, delta in enumerate([c - o for c, o in zip(clamped[1:], network.layers[1:])]):\n",
    "        if i + 1 < len(network.biases):\n",
    "            network.biases[i + 1] += alphas[i] * delta.mean(0)\n",
    "        if i < len(network.weights):\n",
    "            network.weights[i] += alphas[i] * network.layers[i].T @ delta / network.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ExternalWorld:\n",
    "    def __init__(self, x_values, y_values):\n",
    "        self.x = torch.tensor(x_values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_values, dtype=torch.int64)\n",
    "        self.size_dataset = len(self.x)\n",
    "\n",
    "xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "xor_y = np.array([0, 1, 1, 0], dtype=np.int64)\n",
    "xor_world = ExternalWorld(xor_x, xor_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Sliders for 4 weights and 2 biases\n",
    "w_sliders = [widgets.FloatSlider(value=0.0, min=-2, max=2, step=0.1, description=f'w{i}') for i in range(4)]\n",
    "b_sliders = [widgets.FloatSlider(value=1.0, min=-2, max=2, step=0.1, description=f'b{i}') for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xor_trial(w0, w1, w2, w3, b0, b1, plot=True):\n",
    "    init = torch.tensor([w0, w1, w2, w3, b0, b1])\n",
    "    xor_net = Network(\n",
    "        name=\"xor_trial\",\n",
    "        external_world=xor_world,\n",
    "        hyper={\n",
    "            \"hidden_sizes\": [2, 1],\n",
    "            \"output_size\": 2,\n",
    "            \"batch_size\": 4,\n",
    "            \"n_epochs\": 20,\n",
    "            \"n_it_neg\": 1,\n",
    "            \"n_it_pos\": 1,\n",
    "            \"alphas\": [0.1, 0.1, 0.01],\n",
    "            \"beta\": 0.1,\n",
    "            \"activation\": torch.relu\n",
    "        },\n",
    "        neg_phase=ep_neg,\n",
    "        pos_phase=ep_pos,\n",
    "        relax_fn=relax\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xor_net.weights[0][:] = init[0:4].reshape(2, 2)\n",
    "        xor_net.weights[1][:] = init[4:6].reshape(2, 1)\n",
    "\n",
    "    error = xor_net.train(plot=plot)\n",
    "    print(\"Final XOR Error:\", error)\n",
    "    xor_net.update_mini_batch_index(0)\n",
    "    print(\"Output logits:\", xor_net.layers[-1].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2649709534bdc97c848c1fb5d0515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=0.0, description='w0', max=2.0, min=-2.0), FloatSlider(value=0.0, descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7940f1c2eea74920af2a1c13ebb1a139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ui = widgets.VBox(w_sliders + b_sliders)\n",
    "out = widgets.interactive_output(run_xor_trial, {\n",
    "    'w0': w_sliders[0], 'w1': w_sliders[1],\n",
    "    'w2': w_sliders[2], 'w3': w_sliders[3],\n",
    "    'b0': b_sliders[0], 'b1': b_sliders[1]\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
